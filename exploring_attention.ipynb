{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthiasdellago/nanoGPT/blob/master/exploring_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "cSCmDpihq_O6",
        "outputId": "794e86fe-01a6-4e5b-d85b-db80fee4feb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-07 11:28:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-07 11:28:35 (53.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icecream"
      ],
      "metadata": {
        "id": "f3BqbaywvUJY",
        "outputId": "5135d80c-d21c-48ad-ca75-85b6d4378748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting icecream\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from icecream) (2.6.1)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.0.1->icecream) (1.15.0)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream\n",
            "Successfully installed asttokens-2.2.1 colorama-0.4.6 executing-1.2.0 icecream-2.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from icecream import ic\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 4#16 # how many independent sequences will we process in parallel?\n",
        "block_size = 16#32 # what is the maximum context length for predictions?\n",
        "max_iters = 1#5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 16#64\n",
        "n_head = 2#4\n",
        "n_layer = 2#4\n",
        "dropout = 0.0\n",
        "att_iterations = 100\n",
        "testing = False\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "        # make an extra forward-like function that just computes and returns the iterated attention.\n",
        "        # Let's do some statistics on the attrocors in the attention layer.\n",
        "        # The plan is:\n",
        "        # 1. apply the attention operation to each attention vector a few times\n",
        "        # 2. calculate the distances amongst all the attracted (hopefully converged vectors)\n",
        "        # 3. Create a histogram of all the distances.\n",
        "        # Expectation: Some distribution with a peak around (?) the average distance on\n",
        "        # the unit sphere in n dimensions \n",
        "        # (https://math.stackexchange.com/questions/2366580/whats-the-average-euclidian-distance-between-two-points-on-a-unit-n-sphere) \n",
        "        # and maybe a peak at 0.\n",
        "        # If there are entries close to 0 then not every attention vector gets its own attractor!\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.record = False\n",
        "\n",
        "    def set_record(self, record):\n",
        "        self.record = record\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def attention(self, frame, start, values):\n",
        "        # another way of computing attention:\n",
        "        # instead of attention Softmax(<Kx, Qx>/sqrt(d))*Vx\n",
        "        # we compute Softmax(<Q^TKx, x>/sqrt(d))*Vx\n",
        "        # computationally this is less efficient and conditioned.\n",
        "        # The andvantage is that this is conceptually closer to an intuitive understanding\n",
        "        # of attention as a vector field.\n",
        "        # If we define our attention frame=x,\n",
        "        # the points to be subjected to the attention opration as start=Q^TKx,\n",
        "        # and the vectors to be weightes as values=Vx, we recover standard attention, (up to a conditioning error).\n",
        "\n",
        "        B,T,C = start.shape\n",
        "        \n",
        "        #compute scalar product \n",
        "        wei = start @ frame.transpose(-2,-1)\n",
        "        \n",
        "        wei = wei * C**-0.5 # normalise by dimension\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        out = wei @ values # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei * C**-0.5 # normalise by dimension\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        #wei = self.dropout(wei) # This might be important???!!!\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        #experiment section: won't affect output and gradients\n",
        "        with torch.no_grad():\n",
        "          #test attention implementation\n",
        "          if testing:\n",
        "            # comute the same thing with attention, to check implementation:\n",
        "            # apply all the key and query matrix to one side of the scalar product\n",
        "            start = x @ self.query.weight.T @ self.key.weight\n",
        "            att_out = self.attention(frame=x, start=start, values=self.value(x))\n",
        "            #check if you get the same thing. If not throw error.\n",
        "            if not torch.allclose(att_out,out,atol=1e-06):\n",
        "              ic(att_out)\n",
        "              ic(out)\n",
        "              ic(torch.eq(att_out, out))\n",
        "              raise Exception(\"Attention not implemented correctly!\")\n",
        "\n",
        "          if self.record:\n",
        "            print(\"recording:\")\n",
        "            # iterate attention:\n",
        "            # simple self attention with KQ but no V Matrix\n",
        "            # we need to plug in the starting vectors that are created by K and Q^T. They're not x!!\n",
        "            start = x @ self.query.weight.T @ self.key.weight\n",
        "            positions = self.attention(frame=x,start=start,values=x)\n",
        "            \n",
        "            # let the  attention do its thing and it iterate a few times.\n",
        "            # we expect fast convergence due to exponentials.\n",
        "            \n",
        "            # track the delta to see how fast they converge.\n",
        "            max_delta = []\n",
        "            \n",
        "            for _ in range(att_iterations):\n",
        "              new_positions = self.attention(frame=x,start=positions,values=x)\n",
        "              #let's see if they converged: do done more iteration and see how much they move\n",
        "              delta=torch.subtract(positions, new_positions).abs()\n",
        "              max_delta.append(delta.max())\n",
        "              positions = new_positions\n",
        "            \n",
        "            # show convergence!\n",
        "            plt.plot(max_delta)\n",
        "            \n",
        "            #let's see if they converged: do done more iteration and see how much they move\n",
        "            #diff=torch.subtract(start, self.attention(frame=x,start=position,values=x))\n",
        "            #print(diff.max())\n",
        "            #histo=torch.histogram(diff, bins=100)\n",
        "            #print(histo.hist)\n",
        "            # normalize histogram\n",
        "            # hist = histo.hist.div(histo.hist.sum())\n",
        "            #plt.bar(histo.bin_edges[:-1], histo.hist, align='edge', alpha=.4) # popping the last element from bin edges is an ugly hack. fix later\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.record = False\n",
        "\n",
        "    def set_record(self, record):\n",
        "        self.record = record\n",
        "        for head in self.heads:\n",
        "          head.set_record(record)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.record = False\n",
        "\n",
        "    def set_record(self, record):\n",
        "        self.record = record\n",
        "        self.sa.set_record(record)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        \n",
        "        self.record = False\n",
        "\n",
        "    def set_record(self, record):\n",
        "        self.record = record\n",
        "        for block in self.blocks:\n",
        "          block.set_record(record)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for i in range(max_new_tokens):\n",
        "            if i == max_new_tokens-1:\n",
        "              self.set_record(True)\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "\n",
        "m.set_record(False)\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "hoelkOrFY8bN",
        "outputId": "fdec74f7-e041-4b72-a53e-df9e978380f5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008897 M parameters\n",
            "step 0: train loss 4.3390, val loss 4.3295\n",
            "recording:\n",
            "recording:\n",
            "recording:\n",
            "recording:\n",
            "\n",
            "p'olRmPgVUz-BgQoQgjcE!Ta,ZauehsdPrsbab.jSxsIx'JTaPolOm?CGuH3p$eySMqjTB'rD3O\n",
            "qdQ?ye!sFi,tWKgeev.:uejqCeyYqrSw&ccZnVDdCvKn'YHGeQ'YLtFHlv,YAhCAv,pIcQjysBcaCc'qGhChyvp3UViYhaB'smO;KO?e'WngaVjjScOf!sQnvPLDMZa:wesOfbrzw'rNDeqQSUvEalgSWesoBhaC\n",
            "gis?aQm,ul'Fe-LQil,.y&yYEmYPbAcisdnUmfnBNx!LPPdop!s.HqIjTSmrmO$wNFhrxusg3g.sQv!CzsvqyFswelo,eCew?t$'ZHvm:jwN:,W z sewThm!KZNiuYvzxUuuZqQRHPxicc$'cvKRqAjf.s$RobN!qJvoWJOH;.EMaaQmOyeSHqQ3DMbKoW.X&.TfrDOwuOIuc'esOZPjv!oQ'lG&poK&3&'uV&'opXaLs.que$Kz'lFPzC&eFOHysv\n",
            "nq!!GfTalODg.qBUzXANJsRPs-Lsrwg?B'hRbyyStMrqA:Of''DkNEeNHu\n",
            "vOAwCHj o&QfQo?EFjwCsDqC$Yvztrs!AhFT?u;bVIsJx'q:rZl:JUQLm;Utn$qe sT'YugNBot,OThoGJuXl sCgUvmiLSjBbIUQgSEl3$XWszYCyPuumDDC$,GurPn.:KoK.h?wHjVHzvu,FFyUD''FXwH uuQ! cjqhdgf,P'$UO\n",
            "-OI,wtFeFO?yOmkK YuWE'c!iODqc-&Oeyusi;HNJ:qZe:uMyeSRQumD!$q.FTKjBK'CDCnW$ YvMAOpAyyWKbKo:SqC3I!MA$FvaFev-iu\n",
            "eryjv:h!Os oeRL!s&evwukeePs&sm,TAN!e,bicJOFc,yn:Jz-KCQCC'L.,rujA'DQoiQJR!aqJPJvjaN:ZOhZqSXgeBwgxtslYYswPsZHIUcOLy:HCCy?QdGuOec,sO\n",
            "AS$YHgdCPvDUedpSt'GORe$NBKCwjydugMSvhuXWe&;mQmnRIwwgC:;myO?u. AeJszoPMFvH$YprD rAPcyo,XQgmKmuoS:fSlojmnCx$nGeVsu$DE;ogDZw D,ecTh'vwM&:FA:?eaH'mlrezFyksYly:wsgPjRaibMwo Z?aG3Ye-NSZ!lxANZXkYK;HKSTEqDKsQlnJrWdCIzBAwqf\n",
            "3qaLowductmAE'greHWO3qBUvTclkO'pznYw!nsGa$r VRmsAjEYp!s$raT?Ae.W$'hZCsYKqQC$P$qjbL:F.,g3wPWXjleRsLqOSgyDD.v;TVqXOP:I'epvWOsZ'Oseaewr3$cAnVx M?x'cP;HIMOwrdOZv3AqAcNJ LQOs-x:Ui:J$sD?qOYJZTzMO'jOVF;K$o:ejPs .lvzNDdEazoju!'xFwsjpeKbDqqBoquXmEMhPFgDW'p'yvEnGej,tyHyg\n",
            "CHQc'jw?m'Qq,;-krxespHmvgemusmIAxeRgvYQe?Y;sK?QsvZrO\n",
            "OK,aBs!PKCFOFGeKrArvLR.Dq3nCQFC!YDPOPms.RgecyxhIhzwqGM!-orgmbV.sCsVDUg3urcYGzR'my&vOtsHsv$gw DGe,s$RsZSdVXXj ImYOabQsYGv;JXKgmKvGMaZeNshsVQyuLj:SUtpqXweDZygwfSsKsvBPKCSTa MPwrGC?:YOresXwStwAsCb3JSmvDrhpLlUnKCeDl:DyFVLPaPgMgyw.eNMHCfYcdCqXcwePjjGPOJ:w$EkuXxUFw'wCqKe-&HsNt\n",
            "UeHAWCOAeQh&OVa'dA'AwwTqahO!GTWgFhsgugSIcvHJMlhLGHx',Knel,ctfNMGFTquHq:Fqr$WrN,lltwvOS:KezzkbbKgTVgJBlUSbbGs,z$snStorYe;jTyeqjOwm-oWwHnAwexrwjl FpNH.&T,w:v$S,.Ty;HHyl-isQeqrvOMLu?NNCPLFHPhctojcX3YI zeqDCGneqQngycn:&Ycqm$QTr'q:$$\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bs1igIQgeyrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}